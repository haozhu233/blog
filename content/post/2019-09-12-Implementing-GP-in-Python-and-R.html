---
title: "Implementing Gaussian Process in Python and R"
author: Hao Zhu
date: "2019-09-14"
output: html_document
slug: gp_python_r
categories:
  - Deep Learning
tags:
  - R
  - Python
  - Deep Learning
  - Gaussian Process
---



<pre class="r"><code>library(reticulate)
reticulate::use_condaenv(&quot;dl&quot;)
knitr::opts_chunk$set(message = F, warning = F)</code></pre>
<p>This blog post is trying to implementing Gaussian Process (GP) in both Python and R. The main purpose is for my personal practice and hopefully it can also be a reference for future me and other people. In fact, it’s actually converted from my first homework in a <a href="https://www.cs.tufts.edu/comp/150BDL/2019f/">Bayesian Deep Learning class</a>.</p>
<p>A interesting finding is that the implementation in R is much faster than the one using <code>numpy</code>.</p>
<p>All of the equations or figures mentioned in this post can be referened in the <a href="http://www.gaussianprocess.org/gpml/chapters/">Rasmussen &amp; Williams’ textbook for Gaussian Process</a>.</p>
<div id="background" class="section level1">
<h1>Background</h1>
<p>Gaussian Process (GP) can be represented in the form of</p>
<p><span class="math inline">\(f(\mathbf{x}) \sim \mathcal{GP}(m(\mathbf{x}), k(\mathbf{x}, \mathbf{x&#39;}))\)</span></p>
<p>where <span class="math inline">\(m(\mathbf{x})\)</span> is the mean function and <span class="math inline">\(k(\mathbf{x}, \mathbf{x&#39;})\)</span> is the covariance/kernel function. In this post, we are trying to create some kernel functions from scratch. We will also create methods to sample values from the prior and the posterior.</p>
</div>
<div id="mean-kernel-functions" class="section level1">
<h1>Mean &amp; kernel functions</h1>
<p>For simplicity, our mean function is set to be 0 for all x inputs.</p>
<p><span class="math inline">\(m(\textbf{x}) = 0\)</span></p>
<p>Squared Exponential kernel (Eq. 4.9)</p>
<p><span class="math inline">\(k_{SE}(r) = exp(-\frac{r^2}{2l^2})\)</span></p>
<p>Matern kernel (Eq. 4.14)</p>
<p><span class="math inline">\(k_{Matern}(r) = \frac{2^{1-\nu}}{\Gamma(\nu)}(\frac{\sqrt{2\nu}r}{\ell})^{\nu}K_{\nu}(\frac{\sqrt{2\nu}r}{\ell})\)</span></p>
<div style="display: inline-block;vertical-align:top; width: 48%;">
<h2 id="implementations-in-python">Implementations in Python</h2>
<pre class="python"><code>import numpy as np
from scipy.special import gamma, kv
import matplotlib.pyplot as plt

def mean_func(x):
    return np.zeros(len(x))
    
def get_r(x1, x2):
    return np.subtract.outer(x1, x2)

def sqexp_kernel(r, l = 1):
    return np.exp(-0.5 * (r/l)**2)

def matern_kernel(r, l = 1, v = 1):
    r = np.abs(r)
    r[r == 0] = 1e-8
    part1 = 2 ** (1 - v) / gamma(v)
    part2 = (np.sqrt(2 * v) * r / l) ** v
    part3 = kv(v, np.sqrt(2 * v) * r / l)
    return part1 * part2 * part3</code></pre>
</div>
<div style="display: inline-block;vertical-align:top; width: 48%;">
<h2 id="implementations-in-r">Implementations in R</h2>
<pre class="r"><code>library(MASS)
library(plotly)

mean_func &lt;- function(x) {
  rep(0, length(x))
}

get_r &lt;- function(x1, x2) {
  outer(x1, x2, FUN = &quot;-&quot;)
}

sqexp_kernel &lt;- function(r, l = 1) {
  exp(-0.5 * (r/l)^2)
}

matern_kernel &lt;- function(r, l = 1, v = 1) {
  r &lt;- abs(r)
  r[r == 0] &lt;- 1e-8
  part1 = 2^(1-v) / gamma(v)
  part2 = (sqrt(2*v) * r / l)^v
  part3 = besselK(sqrt(2*v) * r / l, nu = v)
  return(part1 * part2 * part3)
}</code></pre>
</div>
<p>Now let’s try to recreate the input-distance to covariance figure using the functions we defined here. Our result (done in python for my homework) is the same as the figures (e.g. Figure 4.1) in the R&amp;W textbook.</p>
<p><img src="/post/2019-09-12-Implementing-GP-in-Python-and-R_files/figure-html/unnamed-chunk-4-1.png" width="960" /></p>
<div id="comparing-the-kernel-performance" class="section level2">
<h2>Comparing the kernel performance</h2>
<div style="display: inline-block;vertical-align:top; width: 48%;">
<h3 id="python-performance">Python performance</h3>
<pre class="python"><code>from time import process_time
x1 = np.random.rand(100)
x2 = np.random.rand(100)
start = process_time()
for i in range(50): sqexp_test = sqexp_kernel(get_r(x1, x2))
print(&quot;SE kernel for rand:&quot; + str((process_time() - start) / 50), &quot; s&quot;)</code></pre>
<pre><code>## SE kernel for rand:0.0005993999999999921  s</code></pre>
<pre class="python"><code>start = process_time()
for i in range(50): matern_test = matern_kernel(get_r(x1, x2))
print(&quot;Matern kernel for rand:&quot; + str((process_time() - start) / 50), &quot; s&quot;)</code></pre>
<pre><code>## Matern kernel for rand:0.005933720000000004  s</code></pre>
</div>
<div style="display: inline-block;vertical-align:top; width: 48%;">
<h3 id="r-performance">R performance</h3>
<pre class="r"><code>x1 = runif(100)
x2 = runif(100)
start = Sys.time()
for(i in 1:50) se_test = sqexp_kernel(get_r(x1, x2))
print(paste(&quot;SE kernel for rand:&quot;, (Sys.time() - start)/ 100, &quot;s&quot;))</code></pre>
<pre><code>## [1] &quot;SE kernel for rand: 0.000440530776977539 s&quot;</code></pre>
<pre class="r"><code>start = Sys.time()
for(i in 1:50) matern_test = matern_kernel(get_r(x1, x2))
print(paste(&quot;Matern kernel for rand:&quot;, (Sys.time() - start)/ 100, &quot;s&quot;))</code></pre>
<pre><code>## [1] &quot;Matern kernel for rand: 0.00153795003890991 s&quot;</code></pre>
</div>
<p>The results are quite comparable. I also checked the performance when it scales up, it’s still quite similar. In fact, especially for Matern kernel, when the size of the input vectors get big, I feel like it’s slightly faster to do it in R.</p>
</div>
</div>
<div id="sampling-function" class="section level1">
<h1>Sampling function</h1>
<div id="sampling-from-the-prior" class="section level2">
<h2>Sampling from the Prior</h2>
<div style="display: inline-block;vertical-align:top; width: 48%;">
<h3 id="implementations-in-python-1">Implementations in Python</h3>
<pre class="python"><code>def sample_prior(
        x, mean_func, cov_func, cov_args = {}, 
        random_seed = -1, n_samples = 5):
    x_mean = mean_func(x)
    x_cov = cov_func(get_r(x, x), **cov_args)
    random_seed = int(random_seed)
    if random_seed &lt; 0:
        prng = np.random
    else:
        prng = np.random.RandomState(random_seed)
    out = prng.multivariate_normal(x_mean, x_cov, n_samples)
    return out</code></pre>
</div>
<div style="display: inline-block;vertical-align:top; width: 48%;">
<h3 id="implementations-in-r-1">Implementations in R</h3>
<pre class="r"><code>sample_prior &lt;- function(
  x, mean_func, cov_func, ..., random_seed = -1, n_samples = 5
) {
  x_mean &lt;- mean_func(x)
  x_cov &lt;- cov_func(get_r(x, x), ...)
  random_seed &lt;- as.integer(random_seed)
  if (random_seed &gt; 0) set.seed(random_seed)
  return(MASS::mvrnorm(n_sample, x_mean, x_cov))
}</code></pre>
</div>
<p>Let’s try to get a few samples from the prior with SE kernel at different length-scales <span class="math inline">\(\ell\)</span>. This time, let’s do it in python.</p>
<pre class="python"><code>plt.figure(figsize = (10, 3))
x = np.linspace(-20, 20, 200)
for l_i, l in enumerate([0.25, 1.0, 4.0]):
    y = sample_prior(x, mean_func, sqexp_kernel, 
                     {&quot;l&quot;: l}, n_samples = 5)
    plt.subplot(131 + l_i)
    for i in range(5):
        plt.plot(x, y[i, :], &quot;-&quot;, alpha = 0.3)
    plt.title(&quot;$\\ell$ = &quot; + str(l))</code></pre>
<p><img src="/post/2019-09-12-Implementing-GP-in-Python-and-R_files/figure-html/unnamed-chunk-9-1.png" width="960" /></p>
</div>
<div id="sampling-from-the-posterior" class="section level2">
<h2>Sampling from the Posterior</h2>
<p>Here we will implement “Prediction using Noisy Observations” because the Noise-free version can be understood as a special case of the noisy one with <span class="math inline">\(\sigma_n = 0\)</span>.</p>
<div style="display: inline-block;vertical-align:top; width: 48%;">
<h3 id="implementations-in-python-2">Implementations in Python</h3>
<pre class="python"><code>def sample_posterior(
    x, y, x_star, mean_func, cov_func, cov_args = {},
    sigma_n = 0.1, random_seed = -1, n_samples = 5
):
    k_xx = cov_func(get_r(x, x), **cov_args)
    k_xxs = cov_func(get_r(x, x_star), **cov_args)
    k_xsx = cov_func(get_r(x_star, x), **cov_args)
    k_xsxs = cov_func(get_r(x_star, x_star), **cov_args)
    
    I = np.identity(k_xx.shape[1])
    k_xx_noise = np.linalg.inv(k_xx + sigma ** 2 * I)
    kxsx_kxxNoise = np.matmul(k_xsx, k_xx_noise)
    # Eq.2.23, 24
    fsb = np.matmul(kxsx_kxxNoise, y_train_N)
    cov_fs = k_xsxs - np.matmul(kxsx_kxxNoise, k_xxs)
    random_seed = int(random_seed)
    if random_seed &lt; 0:
        prng = np.random
    else:
        prng = np.random.RandomState(random_seed)
    out = prng.multivariate_normal(fsb, cov_fs, n_samples)
    return out, fsb, cov_fs</code></pre>
</div>
<div style="display: inline-block;vertical-align:top; width: 48%;">
<h3 id="implementations-in-r-2">Implementations in R</h3>
<pre class="r"><code>sample_posterior &lt;- function(
  x, y, x_star, mean_func, cov_func, ...,
  sigma_n = 0.1, random_seed = -1, n_samples = 5
) {
  k_xx = cov_func(get_r(x, x), ...)
  k_xxs = cov_func(get_r(x, x_star), ...)
  k_xsx = cov_func(get_r(x_star, x), ...)
  k_xsxs = cov_func(get_r(x_star, x_star), ...)
  
  I = diag(1, dim(k_xx)[1])
  k_xx_noise = solve(k_xx + sigma_n ^ 2 * I)
  kxsx_kxxNoise = k_xsx %*% k_xx_noise
  # Eq.2.23, 24
  fsb = kxsx_kxxNoise %*% y
  cov_fs = k_xsxs - kxsx_kxxNoise %*% k_xxs
  random_seed &lt;- as.integer(random_seed)
  if (random_seed &gt; 0) set.seed(random_seed)
  return(MASS::mvrnorm(n_samples, fsb, cov_fs))
}</code></pre>
</div>
<p>This time let’s try to fit some points in R.</p>
<pre class="r"><code>x_train &lt;- c(-10, -8, -5, 3, 7, 15)
y_train &lt;- c(-5, -2, 3, 3, 2, 5)
x_star &lt;- seq(-20, 20, length = 200)

par(mfrow = c(3, 3))
l &lt;- c(0.25, 1, 4)
v &lt;- c(0.5, 2, 8)
for (i in seq(length(l))) {
  for (j in seq(length(v))) {
    dt &lt;- sample_posterior(
      x_train, y_train, x_star, mean_func, matern_kernel, 
      l = l[i], v = v[j], n_samples = 10, random_seed = 100
    )
    matplot(x_star, t(dt), &quot;l&quot;, col = rgb(0.3, 0.3, 0.3, 0.3), lty = 1,
            main = paste(&quot;l=&quot;, l[i], &quot;,v=&quot;, v[j]), ylim = c(-8, 8),
            xlab = &quot;x&quot;, ylab = &quot;y&quot;)
    points(x = x_train, y = y_train, pch = 16, col = &quot;red&quot;)
  }
}</code></pre>
<p><img src="/post/2019-09-12-Implementing-GP-in-Python-and-R_files/figure-html/unnamed-chunk-12-1.png" width="768" /></p>
<p>Note, that when <span class="math inline">\(\ell\)</span> is small, it is easier for the predicted posterior to return to normal (prior), which is the mean function, 0 (see the points around x = 0). As <span class="math inline">\(\ell\)</span> increases, it becomes more and more likely the predicted <span class="math inline">\(y_{x=0}\)</span> to stay at the “local” value, which is provided by the nearest neighbor in <code>y</code>.</p>
</div>
</div>
