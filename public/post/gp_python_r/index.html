<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="author" content="Hao Zhu">
    <meta name="description" content="Hao&#39;s Blog">
    <meta name="keywords" content="blog,developer,personal">

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Implementing Gaussian Process in Python and R"/>
<meta name="twitter:description" content="library(reticulate) reticulate::use_condaenv(&quot;dl&quot;) knitr::opts_chunk$set(message = F, warning = F) This blog post is trying to implementing Gaussian Process (GP) in both Python and R. The main purpose is for my personal practice and hopefully it can also be a reference for future me and other people. In fact, it’s actually converted from my first homework in a Bayesian Deep Learning class.
A interesting finding is that the implementation in R is much faster than the one using numpy."/>


    <base href="https://zhuhao.org/post/gp_python_r/">
    <title>
  Implementing Gaussian Process in Python and R · Hao Zhu
</title>

    <link rel="canonical" href="https://zhuhao.org/post/gp_python_r/">

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700|Merriweather:300,700|Source+Code+Pro:400,700" rel="stylesheet">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css" integrity="sha256-oSrCnRYXvHG31SBifqP2PM1uje7SJUyX0nTwO2RJV54=" crossorigin="anonymous" />

    
      
      
      <link rel="stylesheet" href="https://zhuhao.org/css/coder.min.ac37073bc2826cd28ef57364a9fe339de7ebcb26dafc22fd832cb35cf5b1d048.css" integrity="sha256-rDcHO8KCbNKO9XNkqf4znefryyba/CL9gyyzXPWx0Eg=" crossorigin="anonymous" media="screen" />
    

    

    

    

    <link rel="icon" type="image/png" href="https://zhuhao.org/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="https://zhuhao.org/images/favicon-16x16.png" sizes="16x16">

    

    <meta name="generator" content="Hugo 0.58.1" />
  </head>

  <body class=" ">
    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="https://zhuhao.org/">
      Hao Zhu
    </a>
    <input type="checkbox" id="menu-toggle" />
    <label class="menu-button float-right" for="menu-toggle"><i class="fas fa-bars"></i></label>
    <ul class="navigation-list">
      
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://zhuhao.org/about/">About</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://zhuhao.org/post/">Blog</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://zhuhao.org/projects/">Projects</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://zhuhao.org/contact/">Contact me</a>
          </li>
        
      
      
        
        
        
          
        
          
            
              <li class="navigation-item menu-separator">
                <span>|</span>
              </li>
              
            
            <li class="navigation-item">
              <a href="https://zhuhao.org/cn/"></a>
            </li>
          
        
          
            
            <li class="navigation-item">
              <a href="https://zhuhao.org/zh-cn/">中文</a>
            </li>
          
        
      
    </ul>
  </section>
</nav>


      <div class="content">
        
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">Implementing Gaussian Process in Python and R</h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fas fa-calendar"></i>
              <time datetime='2019-09-12T00:00:00Z'>
                September 12, 2019
              </time>
            </span>
            <span class="reading-time">
              <i class="fas fa-clock"></i>
              7 minutes read
            </span>
          </div>
          <div class="categories">
  <i class="fas fa-folder"></i>
    <a href="https://zhuhao.org/categories/deep-learning/">Deep Learning</a></div>

          <div class="tags">
  <i class="fas fa-tag"></i>
    <a href="https://zhuhao.org/tags/r/">R</a>
      <span class="separator">•</span>
    <a href="https://zhuhao.org/tags/python/">Python</a>
      <span class="separator">•</span>
    <a href="https://zhuhao.org/tags/deep-learning/">Deep Learning</a>
      <span class="separator">•</span>
    <a href="https://zhuhao.org/tags/gaussian-process/">Gaussian Process</a></div>

        </div>
      </header>

      <div>
        


<pre class="r"><code>library(reticulate)
reticulate::use_condaenv(&quot;dl&quot;)
knitr::opts_chunk$set(message = F, warning = F)</code></pre>
<p>This blog post is trying to implementing Gaussian Process (GP) in both Python and R. The main purpose is for my personal practice and hopefully it can also be a reference for future me and other people. In fact, it’s actually converted from my first homework in a <a href="https://www.cs.tufts.edu/comp/150BDL/2019f/">Bayesian Deep Learning class</a>.</p>
<p>A interesting finding is that the implementation in R is much faster than the one using <code>numpy</code>.</p>
<p>All of the equations or figures mentioned in this post can be referened in the <a href="http://www.gaussianprocess.org/gpml/chapters/">Rasmussen &amp; Williams’ textbook for Gaussian Process</a>.</p>
<div id="background" class="section level1">
<h1>Background</h1>
<p>Gaussian Process (GP) can be represented in the form of</p>
<p><span class="math inline">\(f(\mathbf{x}) \sim \mathcal{GP}(m(\mathbf{x}), k(\mathbf{x}, \mathbf{x&#39;}))\)</span></p>
<p>where <span class="math inline">\(m(\mathbf{x})\)</span> is the mean function and <span class="math inline">\(k(\mathbf{x}, \mathbf{x&#39;})\)</span> is the covariance/kernel function. In this post, we are trying to create some kernel functions from scratch. We will also create methods to sample values from the prior and the posterior.</p>
</div>
<div id="mean-kernel-functions" class="section level1">
<h1>Mean &amp; kernel functions</h1>
<p>For simplicity, our mean function is set to be 0 for all x inputs.</p>
<div id="implementations-in-python" class="section level2">
<h2>Implementations in Python</h2>
<pre class="python"><code>import numpy as np
from scipy.special import gamma, kv
import matplotlib.pyplot as plt

def mean_func(x):
    return np.zeros(len(x))
    
# Squared Exponential kernel (Eq. 4.9)
def sqexp_kernel_func(x1, x2, l = 1):
    return np.exp(-0.5 * (np.subtract.outer(x1, x2)/l)**2)
    
# The Matern kernel (Eq. 4.14)
def matern_kernel_func(x1, x2, l = 1, v = 1):
    r = np.abs(np.subtract.outer(x1, x2))
    r[r == 0] = 1e-8
    part1 = 2 ** (1 - v) / gamma(v)
    part2 = (np.sqrt(2 * v) * r / l) ** v
    part3 = kv(v, np.sqrt(2 * v) * r / l)
    return part1 * part2 * part3</code></pre>
<p>Let’s try to recreate the input-distance to covariance figure using the functions we defined here. Our result is the same as the figures (e.g. Figure 4.1) in the R&amp;W textbook.</p>
<pre class="python"><code>sqexp_x1 = np.zeros(30)
sqexp_x2 = np.linspace(0, 3, 30)
def sqexp_kernel_func2(x1, x2, l = 1):
    return np.exp(-0.5 * ((x1 - x2)/l)**2)
sqexp_y_half = sqexp_kernel_func2(sqexp_x1, sqexp_x2, 0.5)
sqexp_y_2 = sqexp_kernel_func2(sqexp_x1, sqexp_x2, 2)
sqexp_y_10 = sqexp_kernel_func2(sqexp_x1, sqexp_x2, 10)

plt.figure(figsize = (10, 3))
plt.subplot(121)
plt.plot(sqexp_x2, sqexp_y_half, &#39;b&#39;, label = &quot;$\\ell$=0.5&quot;)
plt.plot(sqexp_x2, sqexp_y_2, &#39;r:&#39;, label = &quot;$\\ell$=2&quot;)
plt.plot(sqexp_x2, sqexp_y_10, &#39;g:&#39;, label = &quot;$\\ell$=10&quot;)
plt.legend(loc = &quot;upper right&quot;)
plt.xlabel(&quot;input distance, r&quot;)
plt.ylabel(&quot;covariance, k(r)&quot;)
plt.title(&quot;Squared Exponential kernel&quot;)

matern_x1 = np.zeros(30)
matern_x2 = np.linspace(0.00001, 3, 30)
def matern_kernel_func2(x1, x2, l = 1, v = 1):
    r = np.abs(x1 - x2)
    part1 = 2 ** (1 - v) / gamma(v)
    part2 = (np.sqrt(2 * v) * r / l) ** v
    part3 = kv(v, np.sqrt(2 * v) * r / l)
    return part1 * part2 * part3
matern_y_half = matern_kernel_func2(matern_x1, matern_x2, 1, 0.5)
matern_y_2 = matern_kernel_func2(matern_x1, matern_x2, 1, 2)
matern_y_10 = matern_kernel_func2(matern_x1, matern_x2, 1, 10)

plt.subplot(122)
plt.plot(matern_x2, matern_y_half, &#39;b&#39;, label = &quot;$\\ell$=1,$\\nu$=0.5&quot;)
plt.plot(matern_x2, matern_y_2, &#39;r:&#39;, label = &quot;$\\ell$=1,$\\nu$=2&quot;)
plt.plot(matern_x2, matern_y_10, &#39;g:&#39;, label = &quot;$\\ell$=1,$\\nu$=10&quot;)
plt.legend(loc = &quot;upper right&quot;)
plt.xlabel(&quot;input distance, r&quot;)
plt.ylabel(&quot;covariance, k(r)&quot;)
plt.title(&quot;Matern kernel&quot;)
plt.show()</code></pre>
<p><img src="https://zhuhao.org/post/2019-09-12-Implementing-GP-in-R-and-Python_files/figure-html/unnamed-chunk-3-1.png" width="960" /></p>
</div>
<div id="implementation-in-r" class="section level2">
<h2>Implementation in R</h2>
<p>Now let’s try to do the same thing in R.</p>
</div>
</div>
<div id="problem-1-sampling-from-the-prior" class="section level1">
<h1>Problem 1: Sampling from the Prior</h1>
<p>For Problem 1, I used the code skeleton provided on the course website. I also added a <code>cov_args</code> argument and used the <code>Key Word Arguments</code> in the <code>cov_func</code> call so it is more flexible to call different arguments in those covariance functions.</p>
<pre class="python"><code>def draw_GP_prior_samples_at_x_grid(
        x_grid_G, mean_func, cov_func, cov_args = {}, 
        random_seed = 42,
        n_samples = 5):
    x_mean = mean_func(x_grid_G)
    x_cov = cov_func(x_grid_G, x_grid_G, **cov_args)

    # Use consistent random number generator for reproducibility
    prng = np.random.RandomState(int(random_seed))
    out = prng.multivariate_normal(x_mean, x_cov, n_samples)
    
    return out</code></pre>
<div id="a-1b" class="section level2">
<h2>1a &amp; 1b</h2>
<div class="figure"><span id="fig:unnamed-chunk-6"></span>
<img src="https://zhuhao.org/post/2019-09-12-Implementing-GP-in-R-and-Python_files/figure-html/unnamed-chunk-6-1.png" alt="Samples from the prior using a squared exponential kernel with different hyperparameter $\ell$" width="960" />
<p class="caption">
Figure 1: Samples from the prior using a squared exponential kernel with different hyperparameter <span class="math inline">\(\ell\)</span>
</p>
</div>
<p>The sampling from the prior using the squared exponential kernel is shown in Figure 3 above. As suggested by the R&amp;W book, hyperparameter length scale <span class="math inline">\(\ell\)</span> could be understood as “the distance you have to move in input space before the function value can change significantly”, which in the end controls the smoothness of the function. As <span class="math inline">\(\ell\)</span> increases, the function will be more smooth.</p>
</div>
<div id="c-1d" class="section level2">
<h2>1c &amp; 1d</h2>
<div class="figure"><span id="fig:unnamed-chunk-7"></span>
<img src="https://zhuhao.org/post/2019-09-12-Implementing-GP-in-R-and-Python_files/figure-html/unnamed-chunk-7-1.png" alt="Samples from the prior using a Matern kernel with different hyperparameter $\ell$ and $\nu$" width="960" />
<p class="caption">
Figure 2: Samples from the prior using a Matern kernel with different hyperparameter <span class="math inline">\(\ell\)</span> and <span class="math inline">\(\nu\)</span>
</p>
</div>
<p>The sampling from the prior using the Matern kernel is shown in Figure 4. The length scale <span class="math inline">\(\ell\)</span> again controls the scales of changes of the covariates. The new hypterparameter <span class="math inline">\(\nu\)</span> controls the sharpness of the ridge of the covariance. Both of these two hyperparameters are related to the smoothness of the function.</p>
</div>
</div>
<div id="problem-2" class="section level1">
<h1>Problem 2</h1>
<p>In Problem 2, we have been given some training data for x and y. We were also told that the relationship between y and x has a noisy term where <span class="math inline">\(\sigma = 0.1\)</span>.</p>
<p>Using Eq. 2.23, 2.24 from R&amp;W Ch.2 and the function skeleton provided, we developed the following functions to predict <span class="math inline">\(y_i\)</span> at given <span class="math inline">\(x_i\)</span> locations.</p>
<pre class="python"><code>x_train_N = np.asarray([-2.,    -1.8,   -1.,  1.,  1.8,     2.])
y_train_N = np.asarray([-3.,  0.2224,    3.,  3.,  0.2224, -3.])

def draw_GP_posterior_samples_at_x_grid(
        x_train_N, y_train_N, x_grid_G, mean_func, cov_func, cov_args = {},
        sigma=0.1,
        random_seed=42,
        n_samples=5):
    # In this case x = x_train_N, xs (x star) = x_grid_G
    k_xx = cov_func(x_train_N, x_train_N, **cov_args)
    k_xxs = cov_func(x_train_N, x_grid_G, **cov_args)
    k_xsx = cov_func(x_grid_G, x_train_N, **cov_args)
    k_xsxs = cov_func(x_grid_G, x_grid_G, **cov_args)
    
    I = np.identity(k_xx.shape[1])
    k_xx_noise = np.linalg.inv(k_xx + sigma ** 2 * I)
    kxsx_kxxNoise = np.matmul(k_xsx, k_xx_noise)
    # Eq.2.23, 24
    fsb = np.matmul(kxsx_kxxNoise, y_train_N)
    cov_fs = k_xsxs - np.matmul(kxsx_kxxNoise, k_xxs)

    # Use consistent random number generator for reproducibility
    prng = np.random.RandomState(int(random_seed))
    out = prng.multivariate_normal(fsb, cov_fs, n_samples)
    
    return out, fsb, cov_fs</code></pre>
<div id="a-2b" class="section level2">
<h2>2a &amp; 2b</h2>
<div class="figure"><span id="fig:unnamed-chunk-9"></span>
<img src="https://zhuhao.org/post/2019-09-12-Implementing-GP-in-R-and-Python_files/figure-html/unnamed-chunk-9-1.png" alt="Samples from the posterior using a Squared Exponential kernel with different hyperparameter $\ell$" width="960" />
<p class="caption">
Figure 3: Samples from the posterior using a Squared Exponential kernel with different hyperparameter <span class="math inline">\(\ell\)</span>
</p>
</div>
<p>The posterior prediction plots using the Squared Exponential kernel are displayed in Figure 5.</p>
<p>In this case, when <span class="math inline">\(\ell\)</span> is small, it is easier for the predicted posterior to return to normal (prior), which is the mean function, 0. As <span class="math inline">\(\ell\)</span> increases, it becomes more and more likely the predicted <span class="math inline">\(y_{x=0}\)</span> to stay at the “local” value, which is provided by the nearest neighbor in <code>y_train_N</code>.</p>
</div>
<div id="c" class="section level2">
<h2>2c</h2>
<div class="figure"><span id="fig:unnamed-chunk-10"></span>
<img src="https://zhuhao.org/post/2019-09-12-Implementing-GP-in-R-and-Python_files/figure-html/unnamed-chunk-10-1.png" alt="Samples from the posterior using a Matern kernel with different hyperparameter $\ell$" width="960" />
<p class="caption">
Figure 4: Samples from the posterior using a Matern kernel with different hyperparameter <span class="math inline">\(\ell\)</span>
</p>
</div>
<p>The posterior prediction plots using the Squared Exponential kernel are displayed in Figure 6.</p>
</div>
<div id="d" class="section level2">
<h2>2d</h2>
<p>Previously in the code for Figure 6 (hidden for cleanliness), we saved the calculated <span class="math inline">\(\overline{f}_*\)</span> and <span class="math inline">\(cov(f_*)\)</span> for the <span class="math inline">\(\ell = 4.0, \nu = 8\)</span> condition in variable <code>matern_mean</code>, <code>matern_cov</code> (this condition happens to be the last one in the loop). Therefore we used these two values (Eq. 2.23 &amp; 2.24 in the R&amp;W book) and calculated out the predicted value when x = +15. The formula and code we used to do the calculation are shown below.</p>
<p><span class="math inline">\(\overline{f}_* = K(X_*,X)[K(X,X)+\sigma_n^2I]^{-1}\textbf{y}\)</span></p>
<pre class="python"><code>idx_15 = np.abs(x_grid_G - 15).argmin()
print(
    &quot;The expected value at x = +15 is &quot; +
    str(np.round(matern_mean[idx_15], 3))
)</code></pre>
<pre><code>## The expected value at x = +15 is -0.412</code></pre>
<p>The calculated result is similar with what we observed in Figure 6.</p>
</div>
<div id="e" class="section level2">
<h2>2e</h2>
<p>As x moves away from the training point, the expected value is also expected to return to normal, which is 0. We have observed that at x = +15, the expected y is already very close to 0. Without any calculation, we can say that when x = +15, y is expected to be at 0.0.</p>
</div>
<div id="f" class="section level2">
<h2>2f</h2>
<p>Similar with question 2d, we can calculate the expected variance at x = +15 using the covariance function we exported earlier. The formula and code we used to do the calculation is shown below.
<span class="math inline">\(cov(f_*) = K(X_*,X_*) - K(X_*,X)[K(X,X)+\sigma_n^2I]^{-1}K(X,X_*)\)</span><br />
<span class="math inline">\(\sigma_*^2 = diag(cov(f_*))\)</span></p>
<pre class="python"><code>print(
    &quot;The expected variance at x = +15 is &quot; +
    str(np.round(np.diag(matern_cov)[idx_15], 3))
)</code></pre>
<pre><code>## The expected variance at x = +15 is 1.0</code></pre>
</div>
<div id="g" class="section level2">
<h2>2g</h2>
<p>By default, the variance calculated from the diagonal of the covariance matrix are range from 0 to 1. This is the reason why we see <span class="math inline">\(\sigma_*^2 \approx 1\)</span> at x = +15. As x moves away from the training data, we can expect the predicted variance at x = +15 gets even closer to 1.</p>
</div>
<div id="h" class="section level2">
<h2>2h</h2>
<p>Based on the equation of GP, the expected value of <span class="math inline">\(f(x)\)</span> at input values <span class="math inline">\(x\)</span> far from the training data should be controlled by the mean function.</p>
<p><span class="math inline">\(f(\mathbf{x}) \sim \mathcal{GP}(m(\mathbf{x}), k(\mathbf{x}, \mathbf{x&#39;}))\)</span></p>
<p>Therefore, if we want to have <span class="math inline">\(f(x \rightarrow \infty) = 5.0\)</span>, we just need to have a fixed value of 5.0 for the mean function.</p>
<p><span class="math inline">\(m(\textbf{x}) = 5.0\)</span></p>
</div>
</div>
<div id="i" class="section level1">
<h1>2i</h1>
<p>By default the default variance for <span class="math inline">\(f(x)\)</span> should equal to 1.0 for all x far from the training data. From Eq. 2.31 in the R&amp;W book, we see that we can adjust this amount by multiplying the covarience term with a signal variance term <span class="math inline">\(\sigma_f^2\)</span>:</p>
<p><span class="math inline">\(k_{sq-exp}(x_p, x_q) = \sigma_f^2 exp(- \frac{1}{2\ell^2}(x_p - x_q)^2) +\sigma_n^2\delta_pq\)</span></p>
<p>We can do a similar thing for the Matern kernel. Although in the current implementation of the <code>draw_GP_posterior_samples_at_x_grid</code>, we don’t have this signal variance term, it should be easy to add <span class="math inline">\(\sigma_f^2\)</span> to our function and set <code>sigma_f = 4.0</code> in this case.</p>
</div>

      </div>

      <footer>
        <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "haozhu233" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
      </footer>
    </article>

    
  </section>

      </div>

      <footer class="footer">
  <section class="container">
    
     © 2019
    
       · 
      Powered by <a href="https://gohugo.io/">Hugo</a>, <a href="https://github.com/luizdepra/hugo-coder/">Coder</a> & <a href="https://github.com/rstudio/blogdown">blogdown</a>.
    
    
  </section>
</footer>

    </main>

    
    
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </body>
</html>
